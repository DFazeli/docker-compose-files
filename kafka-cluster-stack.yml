# docker swarm init 
# docker swarm join-token manager
# docker swarm join-token worker
# docker swarm leave 3q5y8mi0vwd77tp0cmpbtcii0 [node id]
# docker node update --label-add node1=kafka1 olip8tm3mqn7mxg4e875xbwfe [Node id]
# docker node update --label-add node2=kafka2 olip8tm3mqn7mxg4e875xbwbn [Node id]
# docker node update --label-add node3=kafka3 olip8tm3mqn7mxg4e875xbwgh [Node id]
# on node1: mkdir -p /opt/kafka-cluster/kafka_data1 && mkdir -p /opt/kafka-cluster/zookeeper_data1 
# on node2: mkdir -p /opt/kafka-cluster/kafka_data2 && mkdir -p /opt/kafka-cluster/zookeeper_data2
# on node3: mkdir -p /opt/kafka-cluster/kafka_data3 && mkdir -p /opt/kafka-cluster/zookeeper_data3 
# docker stack deploy -c kafka-cluster-stack.yml zk

version: '3.9'
services:

  zk1:
    hostname: zk1
    image: bitnami/zookeeper:latest
    ports:
      - '12181:2181'
      - '12888:2888'
      - '13888:3888'
    deploy:
      mode: replicated
      replicas: 1
      placement:
         constraints:
           - node.labels.node1 == kafka1
    extra_hosts:
      kafka1: 192.168.35.52
      kafka2: 192.168.35.72
      kafka3: 192.168.35.85
      zk1: 192.168.35.52
      zk2: 192.168.35.72
      zk3: 192.168.35.85
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_SERVER_ID=1
      - ZOO_SERVERS=0.0.0.0:2888:3888,zk2:22888:23888,zk3:32888:33888
    volumes:
      - /opt/kafka-cluster/zookeeper_data1:/bitnami/zookeeper/data/

  zk2:
    hostname: zk2
    image: bitnami/zookeeper:latest
    ports:
      - '22181:2181'
      - '22888:2888'
      - '23888:3888'
    deploy:
      mode: replicated
      replicas: 1
      placement:
         constraints:
           - node.labels.node2 == kafka2
    extra_hosts:
      kafka1: 192.168.35.52
      kafka2: 192.168.35.72
      kafka3: 192.168.35.85
      zk1: 192.168.35.52
      zk2: 192.168.35.72
      zk3: 192.168.35.85
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_SERVER_ID=2
      - ZOO_SERVERS=zk1:12888:13888,0.0.0.0:2888:3888,zk3:32888:33888
    volumes:
      - /opt/kafka-cluster/zookeeper_data2:/bitnami/zookeeper/data
  zk3:
    hostname: zk3
    image: bitnami/zookeeper:latest
    ports:
      - '32181:2181'
      - '32888:2888'
      - '33888:3888'
    deploy:
      mode: replicated
      replicas: 1
      placement:
         constraints:
           - node.labels.node3 == kafka3
    extra_hosts:
      kafka1: 192.168.35.52
      kafka2: 192.168.35.72
      kafka3: 192.168.35.85
      zk1: 192.168.35.52
      zk2: 192.168.35.72
      zk3: 192.168.35.85
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_SERVER_ID=3
      - ZOO_SERVERS=zk1:12888:13888,zk2:22888:23888,0.0.0.0:2888:3888
    volumes:
      - /opt/kafka-cluster/zookeeper_data3:/bitnami/zookeeper/data/

  kafka1:
    image: wurstmeister/kafka
    hostname: kafka1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/kafka-cluster/kafka_data1:/kafka
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          cpus: '3'
      mode: replicated
      replicas: 1
      placement:
         constraints:
          - node.labels.node1 == kafka1
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
      - "1099:1099"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zk1:2181,zk2:2181,zk3:2181/kafka"
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://kafka1:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: OUTSIDE
      KAFKA_BROKER_ID: 52
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_DIRS: /kafka/data
      KAFKA_NUM_PARTITIONS: 9
      KAFKA_RECOVERY_THREADS_PER_DATA_DIR: 3
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 3
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka1 -Dcom.sun.management.jmxremote.rmi.port=1099"
      JMX_PORT: 1099
    logging:
       driver: "json-file"
       options:
          max-size: "200k"
          max-file: "10"
    healthcheck:
      test: ["CMD", "bash", "-c", "unset" , "JMX_PORT" ,";" ,"kafka-topics.sh","--zookeeper","zk1:21811","--list"]
      interval: 30s
      timeout: 10s
      retries: 3


  kafka2:
    image: wurstmeister/kafka
    hostname: kafka2
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/kafka-cluster/kafka_data2:/kafka
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          cpus: '3'
      mode: replicated
      replicas: 1
      placement:
         constraints:
          - node.labels.node2 == kafka2
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
      - "2099:1099"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zk1:2181,zk2:2181,zk3:2181/kafka"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://kafka2:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: OUTSIDE
      KAFKA_BROKER_ID: 72
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_DIRS: /kafka/data
      KAFKA_NUM_PARTITIONS: 9
      KAFKA_RECOVERY_THREADS_PER_DATA_DIR: 3
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 3
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka2 -Dcom.sun.management.jmxremote.rmi.port=1099"
      JMX_PORT: 2099
    logging:
       driver: "json-file"
       options:
          max-size: "200k"
          max-file: "10"
    healthcheck:
      test: ["CMD", "bash", "-c", "unset" , "JMX_PORT" ,";" ,"kafka-topics.sh","--zookeeper","zk2:21812","--list"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka3:
    image: wurstmeister/kafka
    hostname: kafka3
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/kafka-cluster/kafka_data3:/kafka
      - /etc/localtime:/etc/localtime:ro
    deploy:
      resources:
        limits:
          cpus: '3'
      mode: replicated
      replicas: 1
      placement:
         constraints:
          - node.labels.node3 == kafka3
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
      - "3099:1099"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zk1:2181,zk2:2181,zk3:2181/kafka"
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://kafka3:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: OUTSIDE
      KAFKA_BROKER_ID: 85
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_DIRS: /kafka/data
      KAFKA_NUM_PARTITIONS: 9
      KAFKA_RECOVERY_THREADS_PER_DATA_DIR: 3
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 3
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka3 -Dcom.sun.management.jmxremote.rmi.port=1099"
      JMX_PORT: 3099
    logging:
       driver: "json-file"
       options:
          max-size: "200k"
          max-file: "10"
    healthcheck:
      test: ["CMD", "bash", "-c", "unset" , "JMX_PORT" ,";" ,"kafka-topics.sh","--zookeeper","zk3:21813","--list"]
      interval: 30s
      timeout: 10s
      retries: 3
      
      
 # ###### Filebeat config sample #########
 filebeat.config.modules:
  path: /etc/filebeat/modules.d/*.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/app.log
  fields:
     log_topic: TopicName

output.kafka:
    hosts: ["kafka1:9094","kafka2:9094","kafka3:9094"]
    topic: '%{[fields.log_topic]}'
    required_acks: 1
    compression: gzip
    max_message_bytes: 1000000

 # ------ /etc/hosts in filebeat server -----
  192.168.35.52 kafka1
  192.168.35.72 kafka2
  192.168.35.85 kafka3

# --#---#---#-- Fluntd-compose.yml --#--#--#--#--#--#--
version: "3.9"
services:
  fluentd-dnsquerylog:
    image: 192.168.35.101:5000/fluentd:1.0
    restart: always
    volumes:
      - ./config/fluentd/fluentd-dnsquerylog.conf:/fluentd/etc/fluent.conf
    deploy:
      resources:
        limits:
          cpus: 2
          memory: 2g
    extra_hosts:
      kafka1: 192.168.35.52
      kafka2: 192.168.35.72
      kafka3: 192.168.35.85
    logging:
       driver: "json-file"
       options:
          max-size: "200k"
          max-file: "10"
 
#--#--#--#--#--# fluntd Config --#--#--#--#--#--#--#--#--
<system>
  workers 3
  log_level error
</system>

<source>
  @type kafka_group
  brokers 192.168.35.52:9094,192.168.35.72:9094,192.168.35.85:9094
  topics topic1
  consumer_group fluentd-topic1-Reader
  start_from_beginning false
  num_threads 3
</source>

<filter topic1>
  @type parser
  key_name message
  reserve_data true
  reserve_time false
  <parse>
     @type regexp
     time_key timestamp
     time_type float
     expression  /\[(?<timestamp>\S*)\] Packet from (?<DNS_Client_IP>[^:]*):(?<DNS_Client_Port>\d*) for (?<DNS_Domain_name>\S*) (?<DNS_Query_Type>\w*) with id (?<DNS_Query_ID>\d*)/i
  </parse>
</filter>

<filter topic1>
  @type record_transformer
  remove_keys @metadata,message,input,fields,agent,ecs,log
  enable_ruby true
</filter>

<match topic1>
  @type stdout
#  @type elasticsearch
#  hosts ES-MASTER01,ES-MASTER02
#  port 9200
#  user elastic
#  password LMcQeFGjp7
#  scheme http
#  include_timestamp true
#  logstash_format false
#  index_name topic1-alias
#  suppress_type_name true
#  prefer_oj_serializer true
#  <buffer tag, time>
#    flush_thread_count 8
#    timekey 1s
#    timekey_wait 0s
#    retry_wait 1.0
#    num_threads 8
#  </buffer>
</match>


#--#--#--#--#-- Kafka-manager Or Cmak for kafka monitoring #--#--#--#--#--
version: '3.8'
services:
  kafka-monitoring:
    container_name: kafka-monitoring
    image: hlebalbau/kafka-manager
    restart: on-failure
    command: -Dconfig.file=/cmak/conf/application.conf -Dapplication.home=/cmak
    environment:
      - ZK_HOSTS=192.168.35.52:22181
    ports:
      - "9001:9000"
    volumes:
      # - cmak_config:/cmak/conf/
       - /etc/localtime:/etc/localtime:ro
    extra_hosts:
      kafka1: 192.168.35.52
      kafka2: 192.168.35.72
      kafka3: 192.168.35.85
volumes:
   cmak_config:
      driver: local
      driver_opts:
        type: 'none'
        o: 'bind'
        device: '/opt/cmak-config'


